[serge]: Collecting llama-cpp-python==0.1.55
[serge]:   Downloading llama_cpp_python-0.1.55.tar.gz (1.4 MB)
[serge]:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/1.4 MB 27.3 MB/s eta 0:00:00
[serge]:   Installing build dependencies: started
[serge]:   Installing build dependencies: finished with status 'done'
[serge]:   Getting requirements to build wheel: started
[serge]:   Getting requirements to build wheel: finished with status 'done'
[serge]:   Preparing metadata (pyproject.toml): started
[serge]:   Preparing metadata (pyproject.toml): finished with status 'done'
[serge]: Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/site-packages (from llama-cpp-python==0.1.55) (4.6.2)
[serge]: Building wheels for collected packages: llama-cpp-python
[serge]:   Building wheel for llama-cpp-python (pyproject.toml): started
[serge]:   Building wheel for llama-cpp-python (pyproject.toml): finished with status 'done'
[serge]:   Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.55-cp311-cp311-linux_x86_64.whl size=207188 sha256=41e3aa0b12ffbd06209016de81f8b9c707fb23bb3d7ad28a3e6e192e2b7b5ef5
[serge]:   Stored in directory: /root/.cache/pip/wheels/a8/66/39/2da84326d8832889630d841945e3194020a93d9702852df1c7
[serge]: Successfully built llama-cpp-python
[serge]: Installing collected packages: llama-cpp-python
[serge]: Successfully installed llama-cpp-python-0.1.55
[serge]: WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
[serge]: 139:C 27 Jun 2023 18:05:26.390 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
[serge]: 139:C 27 Jun 2023 18:05:26.390 # Redis version=7.0.11, bits=64, commit=00000000, modified=0, pid=139, just started
[serge]: 139:C 27 Jun 2023 18:05:26.390 # Configuration loaded
[serge]: 139:M 27 Jun 2023 18:05:26.390 * monotonic clock: POSIX clock_gettime
[serge]: 139:M 27 Jun 2023 18:05:26.390 * Running mode=standalone, port=6379.
[serge]: 139:M 27 Jun 2023 18:05:26.390 # Server initialized
[serge]: 139:M 27 Jun 2023 18:05:26.398 * Creating AOF base file appendonly.aof.1.base.rdb on server start
[serge]: 139:M 27 Jun 2023 18:05:26.402 * Creating AOF incr file appendonly.aof.1.incr.aof on server start
[serge]: 139:M 27 Jun 2023 18:05:26.402 * Ready to accept connections
[serge]: INFO:     Started server process [141]
[serge]: INFO:     Waiting for application startup.
[serge]: 2023-06-27 18:05:26.935 | INFO     | src.serge.main:start_database:85 - initializing models
[serge]: 2023-06-27T18:05:26.935895+0000 INFO initializing models
[serge]: INFO:     Application startup complete.
[serge]: INFO:     Uvicorn running on http://0.0.0.0:8008 (Press CTRL+C to quit)
[serge]: Error: Not found: "/usr/src/app/weights/tokenizer.model": No such file or directory Error #2
[serge]: INFO:     10.42.0.10:50388 - "GET / HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:50388 - "GET /_app/immutable/entry/start.9223bd74.js HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:50388 - "GET /_app/immutable/chunks/index.bf244542.js HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:50402 - "GET /_app/immutable/chunks/singletons.f5e9e7d8.js HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:50406 - "GET /_app/immutable/entry/app.77dae9d2.js HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:50406 - "GET /favicon.png HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:50406 - "GET /_app/immutable/nodes/0.9bf22032.js HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:50388 - "GET /_app/immutable/chunks/navigation.aec8d7f3.js HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:50402 - "GET /_app/immutable/chunks/stores.1864fb40.js HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:50406 - "GET /_app/immutable/assets/0.6f63f997.css HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:50412 - "GET /_app/immutable/nodes/1.79dd03d2.js HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:50424 - "GET /_app/immutable/nodes/2.707c992d.js HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:50402 - "GET /chat/ HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:50402 - "GET /model/all HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:50402 - "GET /_app/immutable/assets/rubik-pixels-latin-400-normal.ae231061.woff2 HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:50402 - "GET /_app/immutable/nodes/4.3421b8a9.js HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:50402 - "GET /model/all HTTP/1.1" 200 OK
[serge]: Downloading tokenizer...
[serge]: INFO:     10.42.0.10:59698 - "GET /model/all HTTP/1.1" 200 OK
[serge]: Downloading GPT4AlpacaLoRA-30B model from TheBloke/gpt4-alpaca-lora-30B-4bit-GGML...
[serge]: INFO:     10.42.0.10:59698 - "GET /model/all HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:59698 - "GET /model/all HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:59698 - "GET /model/all HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:59698 - "GET /model/all HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:59698 - "GET /model/all HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:59698 - "GET /model/all HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:59698 - "GET /model/all HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:59698 - "GET /model/all HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:59698 - "GET /model/all HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:59698 - "GET /model/all HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:59698 - "GET /model/all HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:59698 - "GET /model/all HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:59698 - "GET /model/all HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:59698 - "GET /model/all HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:59698 - "GET /model/all HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:59698 - "GET /model/all HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:59698 - "GET /model/all HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:59698 - "GET /model/all HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:59698 - "GET /model/all HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:59698 - "GET /model/all HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:59698 - "GET /model/all HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:59698 - "GET /model/all HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:59698 - "GET /model/all HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:55948 - "GET /model/all HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:45692 - "GET /models HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:45692 - "GET /chat/ HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:45704 - "GET /model/all HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:45692 - "GET /models HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:45692 - "GET /chat/ HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:45704 - "GET /model/all HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:45704 - "GET /models HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:45704 - "GET /chat/ HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:45692 - "GET /model/all HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:33066 - "GET /models HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:33066 - "GET /chat/ HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:33068 - "GET /model/all HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:33068 - "GET /model/all HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:42554 - "GET /models HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:42554 - "GET /chat/ HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:42568 - "GET /model/all HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:37700 - "GET /models HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:37700 - "GET /chat/ HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:37710 - "GET /model/all HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:37700 - "GET /model/all HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:52228 - "GET /models HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:52228 - "GET /chat/ HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:52242 - "GET /model/all HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:56846 - "GET /models HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:56846 - "GET /chat/ HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:56850 - "GET /model/all HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:56078 - "GET /models HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:56078 - "GET /chat/ HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:56090 - "GET /model/all HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:56090 - "GET /model/all HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:59170 - "GET /models HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:59170 - "GET /model/all HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:59174 - "GET /chat/ HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:34760 - "GET /models HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:34760 - "GET /chat/ HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:34776 - "GET /model/all HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:34760 - "GET /model/all HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:46282 - "GET /models HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:46282 - "GET /chat/ HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:46292 - "GET /model/all HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:46292 - "GET /models HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:46292 - "GET /chat/ HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:46282 - "GET /model/all HTTP/1.1" 200 OK
[serge]: converting /usr/src/app/weights/GPT4AlpacaLoRA-30B.bin
[serge]: File /usr/src/app/weights/GPT4AlpacaLoRA-30B.bin already converted
[serge]: /usr/src/app/weights/GPT4AlpacaLoRA-30B.bin: input ggml has already been converted to 'ggjt' magic
[serge]: 
[serge]: INFO:     10.42.0.10:46282 - "GET /models HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:46282 - "GET /chat/ HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:46292 - "GET /model/all HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:46282 - "GET /model/all HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:45218 - "GET /model/all HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:45224 - "GET /chat/ HTTP/1.1" 200 OK
[serge]: llama.cpp: loading model from /usr/src/app/weights/GPT4AlpacaLoRA-30B.bin
[serge]: llama_model_load_internal: format     = ggjt v3 (latest)
[serge]: llama_model_load_internal: n_vocab    = 32000
[serge]: llama_model_load_internal: n_ctx      = 512
[serge]: llama_model_load_internal: n_embd     = 6656
[serge]: llama_model_load_internal: n_mult     = 256
[serge]: llama_model_load_internal: n_head     = 52
[serge]: llama_model_load_internal: n_layer    = 60
[serge]: llama_model_load_internal: n_rot      = 128
[serge]: llama_model_load_internal: ftype      = 9 (mostly Q5_1)
[serge]: llama_model_load_internal: n_ff       = 17920
[serge]: llama_model_load_internal: n_parts    = 1
[serge]: llama_model_load_internal: model size = 30B
[serge]: llama_model_load_internal: ggml ctx size =    0.13 MB
[serge]: llama_model_load_internal: mem required  = 25573.14 MB (+ 3124.00 MB per state)
[serge]: .
[serge]: llama_init_from_file: kv self size  =  780.00 MB
[serge]: AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
[serge]: INFO:     10.42.0.10:41338 - "POST /chat/?temperature=0.1&top_k=50&max_length=256&top_p=0.95&context_window=512&repeat_last_n=64&model=GPT4AlpacaLoRA-30B&n_threads=4&repeat_penalty=1.3&init_prompt=Below+is+an+instruction+that+describes+a+task.+Write+a+response+that+appropriately+completes+the+request. HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:44948 - "GET /_app/immutable/assets/3.bd923f35.css HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:41338 - "GET /_app/immutable/nodes/3.d28f3de7.js HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:41338 - "GET /chat/8a8c54be-8e73-43df-9e18-eb90c6ed154d HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:41338 - "GET /chat/ HTTP/1.1" 200 OK
[serge]: 2023-06-27 18:26:45.081 | DEBUG    | serge.routers.chat:stream_ask_a_question:143 - Starting redis client
[serge]: 2023-06-27T18:26:45.081886+0000 DEBUG Starting redis client
[serge]: 2023-06-27 18:26:45.083 | DEBUG    | serge.routers.chat:stream_ask_a_question:149 - creating chat
[serge]: 2023-06-27T18:26:45.083380+0000 DEBUG creating chat
[serge]: 2023-06-27 18:26:45.084 | DEBUG    | serge.routers.chat:stream_ask_a_question:153 - model_path='GPT4AlpacaLoRA-30B' n_ctx=512 n_threads=4 last_n_tokens_size=64 max_tokens=256 temperature=0.1 top_p=0.95 repeat_penalty=1.3 top_k=50
[serge]: 2023-06-27T18:26:45.084132+0000 DEBUG model_path='GPT4AlpacaLoRA-30B' n_ctx=512 n_threads=4 last_n_tokens_size=64 max_tokens=256 temperature=0.1 top_p=0.95 repeat_penalty=1.3 top_k=50
[serge]: 2023-06-27 18:26:45.084 | DEBUG    | serge.routers.chat:stream_ask_a_question:154 - creating history
[serge]: 2023-06-27T18:26:45.084471+0000 DEBUG creating history
[serge]: 2023-06-27 18:26:45.084 | DEBUG    | serge.routers.chat:stream_ask_a_question:157 - adding question Hello
[serge]: 
[serge]: 2023-06-27T18:26:45.084981+0000 DEBUG adding question Hello
[serge]: 
[serge]: 2023-06-27 18:26:45.086 | DEBUG    | serge.routers.chat:stream_ask_a_question:163 - creating Llama client
[serge]: 2023-06-27T18:26:45.086170+0000 DEBUG creating Llama client
[serge]: llama.cpp: loading model from /usr/src/app/weights/GPT4AlpacaLoRA-30B.bin
[serge]: llama_model_load_internal: format     = ggjt v3 (latest)
[serge]: llama_model_load_internal: n_vocab    = 32000
[serge]: llama_model_load_internal: n_ctx      = 512
[serge]: llama_model_load_internal: n_embd     = 6656
[serge]: llama_model_load_internal: n_mult     = 256
[serge]: llama_model_load_internal: n_head     = 52
[serge]: llama_model_load_internal: n_layer    = 60
[serge]: llama_model_load_internal: n_rot      = 128
[serge]: llama_model_load_internal: ftype      = 9 (mostly Q5_1)
[serge]: llama_model_load_internal: n_ff       = 17920
[serge]: llama_model_load_internal: n_parts    = 1
[serge]: llama_model_load_internal: model size = 30B
[serge]: llama_model_load_internal: ggml ctx size =    0.13 MB
[serge]: llama_model_load_internal: mem required  = 25573.14 MB (+ 3124.00 MB per state)
[serge]: .
[serge]: llama_init_from_file: kv self size  =  780.00 MB
[serge]: AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
[serge]: INFO:     10.42.0.10:44962 - "GET /chat/8a8c54be-8e73-43df-9e18-eb90c6ed154d/question?prompt=Hello%0A HTTP/1.1" 200 OK
[serge]: 2023-06-27 18:27:14.258 | DEBUG    | serge.routers.chat:stream_ask_a_question:143 - Starting redis client
[serge]: 2023-06-27T18:27:14.258739+0000 DEBUG Starting redis client
[serge]: 2023-06-27 18:27:14.259 | DEBUG    | serge.routers.chat:stream_ask_a_question:149 - creating chat
[serge]: 2023-06-27T18:27:14.259179+0000 DEBUG creating chat
[serge]: 2023-06-27 18:27:14.259 | DEBUG    | serge.routers.chat:stream_ask_a_question:153 - model_path='GPT4AlpacaLoRA-30B' n_ctx=512 n_threads=4 last_n_tokens_size=64 max_tokens=256 temperature=0.1 top_p=0.95 repeat_penalty=1.3 top_k=50
[serge]: 2023-06-27T18:27:14.259472+0000 DEBUG model_path='GPT4AlpacaLoRA-30B' n_ctx=512 n_threads=4 last_n_tokens_size=64 max_tokens=256 temperature=0.1 top_p=0.95 repeat_penalty=1.3 top_k=50
[serge]: 2023-06-27 18:27:14.259 | DEBUG    | serge.routers.chat:stream_ask_a_question:154 - creating history
[serge]: 2023-06-27T18:27:14.259550+0000 DEBUG creating history
[serge]: 2023-06-27 18:27:14.259 | DEBUG    | serge.routers.chat:stream_ask_a_question:157 - adding question What is your name?
[serge]: 2023-06-27T18:27:14.259659+0000 DEBUG adding question What is your name?
[serge]: 2023-06-27 18:27:14.259 | DEBUG    | serge.routers.chat:stream_ask_a_question:163 - creating Llama client
[serge]: 2023-06-27T18:27:14.259972+0000 DEBUG creating Llama client
[serge]: llama.cpp: loading model from /usr/src/app/weights/GPT4AlpacaLoRA-30B.bin
[serge]: llama_model_load_internal: format     = ggjt v3 (latest)
[serge]: llama_model_load_internal: n_vocab    = 32000
[serge]: llama_model_load_internal: n_ctx      = 512
[serge]: llama_model_load_internal: n_embd     = 6656
[serge]: llama_model_load_internal: n_mult     = 256
[serge]: llama_model_load_internal: n_head     = 52
[serge]: llama_model_load_internal: n_layer    = 60
[serge]: llama_model_load_internal: n_rot      = 128
[serge]: llama_model_load_internal: ftype      = 9 (mostly Q5_1)
[serge]: llama_model_load_internal: n_ff       = 17920
[serge]: llama_model_load_internal: n_parts    = 1
[serge]: llama_model_load_internal: model size = 30B
[serge]: llama_model_load_internal: ggml ctx size =    0.13 MB
[serge]: llama_model_load_internal: mem required  = 25573.14 MB (+ 3124.00 MB per state)
[serge]: .
[serge]: llama_init_from_file: kv self size  =  780.00 MB
[serge]: AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
[serge]: INFO:     10.42.0.10:42170 - "GET /chat/8a8c54be-8e73-43df-9e18-eb90c6ed154d/question?prompt=What+is+your+name%3F HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:43772 - "GET /chat/ HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:43772 - "DELETE /chat/8a8c54be-8e73-43df-9e18-eb90c6ed154d HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:43772 - "GET /chat/ HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:43778 - "GET /model/all HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:43778 - "GET /chat/ HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:43778 - "GET /chat/ HTTP/1.1" 200 OK
[serge]: llama.cpp: loading model from /usr/src/app/weights/GPT4AlpacaLoRA-30B.bin
[serge]: llama_model_load_internal: format     = ggjt v3 (latest)
[serge]: llama_model_load_internal: n_vocab    = 32000
[serge]: llama_model_load_internal: n_ctx      = 512
[serge]: llama_model_load_internal: n_embd     = 6656
[serge]: llama_model_load_internal: n_mult     = 256
[serge]: llama_model_load_internal: n_head     = 52
[serge]: llama_model_load_internal: n_layer    = 60
[serge]: llama_model_load_internal: n_rot      = 128
[serge]: llama_model_load_internal: ftype      = 9 (mostly Q5_1)
[serge]: llama_model_load_internal: n_ff       = 17920
[serge]: llama_model_load_internal: n_parts    = 1
[serge]: llama_model_load_internal: model size = 30B
[serge]: llama_model_load_internal: ggml ctx size =    0.13 MB
[serge]: llama_model_load_internal: mem required  = 25573.14 MB (+ 3124.00 MB per state)
[serge]: .
[serge]: llama_init_from_file: kv self size  =  780.00 MB
[serge]: AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
[serge]: 
[serge]: llama_print_timings:        load time = 15319.22 ms
[serge]: llama_print_timings:      sample time =     6.16 ms /    23 runs   (    0.27 ms per token)
[serge]: llama_print_timings: prompt eval time = 15319.20 ms /    53 tokens (  289.04 ms per token)
[serge]: llama_print_timings:        eval time = 14092.71 ms /    22 runs   (  640.58 ms per token)
[serge]: llama_print_timings:       total time = 29798.13 ms
[serge]: INFO:     10.42.0.10:43778 - "POST /chat/?temperature=0.1&top_k=50&max_length=256&top_p=0.95&context_window=512&repeat_last_n=64&model=GPT4AlpacaLoRA-30B&n_threads=4&repeat_penalty=1.3&init_prompt=Below+is+an+instruction+that+describes+a+task.+Write+a+response+that+appropriately+completes+the+request. HTTP/1.1" 200 OK
[serge]: 2023-06-27 18:27:44.481 | INFO     | serge.routers.chat:event_generator:205 - My name is AI, I am here to help you with any tasks or questions you may have. ⁇ 
[serge]: 2023-06-27T18:27:44.481808+0000 INFO My name is AI, I am here to help you with any tasks or questions you may have. ⁇ 
[serge]: INFO:     10.42.0.10:43772 - "GET /chat/1102c8f3-f611-4e28-99d0-fa527ba79551 HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:43778 - "GET /chat/ HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:43778 - "GET /chat/ HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:42170 - "GET /chat/ HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:33694 - "GET /chat/ HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:33708 - "GET /model/all HTTP/1.1" 200 OK
[serge]: llama.cpp: loading model from /usr/src/app/weights/GPT4AlpacaLoRA-30B.bin
[serge]: llama_model_load_internal: format     = ggjt v3 (latest)
[serge]: llama_model_load_internal: n_vocab    = 32000
[serge]: llama_model_load_internal: n_ctx      = 512
[serge]: llama_model_load_internal: n_embd     = 6656
[serge]: llama_model_load_internal: n_mult     = 256
[serge]: llama_model_load_internal: n_head     = 52
[serge]: llama_model_load_internal: n_layer    = 60
[serge]: llama_model_load_internal: n_rot      = 128
[serge]: llama_model_load_internal: ftype      = 9 (mostly Q5_1)
[serge]: llama_model_load_internal: n_ff       = 17920
[serge]: llama_model_load_internal: n_parts    = 1
[serge]: llama_model_load_internal: model size = 30B
[serge]: llama_model_load_internal: ggml ctx size =    0.13 MB
[serge]: llama_model_load_internal: mem required  = 25573.14 MB (+ 3124.00 MB per state)
[serge]: .
[serge]: llama_init_from_file: kv self size  =  780.00 MB
[serge]: AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
[serge]: INFO:     10.42.0.10:33708 - "POST /chat/?temperature=0.1&top_k=50&max_length=256&top_p=0.95&context_window=512&repeat_last_n=64&model=GPT4AlpacaLoRA-30B&n_threads=4&repeat_penalty=1.3&init_prompt=Below+is+an+instruction+that+describes+a+task.+Write+a+response+that+appropriately+completes+the+request. HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:33708 - "GET /chat/c0c38f0a-accc-4960-bf87-e3cdb03fa2c4 HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:33708 - "GET /chat/ HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:33708 - "GET /chat/ HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:33708 - "DELETE /chat/c0c38f0a-accc-4960-bf87-e3cdb03fa2c4 HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:33708 - "GET /chat/ HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:42080 - "GET /model/all HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:33708 - "GET /chat/ HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:33708 - "GET /chat/ HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:42080 - "GET /chat/1102c8f3-f611-4e28-99d0-fa527ba79551 HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:33610 - "GET /chat/ HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:33620 - "GET /model/all HTTP/1.1" 200 OK
[serge]: llama.cpp: loading model from /usr/src/app/weights/GPT4AlpacaLoRA-30B.bin
[serge]: llama_model_load_internal: format     = ggjt v3 (latest)
[serge]: llama_model_load_internal: n_vocab    = 32000
[serge]: llama_model_load_internal: n_ctx      = 512
[serge]: llama_model_load_internal: n_embd     = 6656
[serge]: llama_model_load_internal: n_mult     = 256
[serge]: llama_model_load_internal: n_head     = 52
[serge]: llama_model_load_internal: n_layer    = 60
[serge]: llama_model_load_internal: n_rot      = 128
[serge]: llama_model_load_internal: ftype      = 9 (mostly Q5_1)
[serge]: llama_model_load_internal: n_ff       = 17920
[serge]: llama_model_load_internal: n_parts    = 1
[serge]: llama_model_load_internal: model size = 30B
[serge]: llama_model_load_internal: ggml ctx size =    0.13 MB
[serge]: llama_model_load_internal: mem required  = 25573.14 MB (+ 3124.00 MB per state)
[serge]: .
[serge]: llama_init_from_file: kv self size  =  780.00 MB
[serge]: AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
[serge]: INFO:     10.42.0.10:40846 - "POST /chat/?temperature=0.1&top_k=50&max_length=256&top_p=0.95&context_window=512&repeat_last_n=64&model=GPT4AlpacaLoRA-30B&n_threads=4&repeat_penalty=1.3&init_prompt=Below+is+an+instruction+that+describes+a+task.+Write+a+response+that+appropriately+completes+the+request. HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:40846 - "GET /chat/18773ede-d500-400f-a022-b0944bd848b0 HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:40846 - "GET /chat/ HTTP/1.1" 200 OK
[serge]: 2023-06-27 18:28:55.731 | INFO     | serge.routers.chat:event_generator:205 - Hi, how can I help you? ⁇  ⁇  ⁇ nopeaktexttoskreplytexttoreplytexttotexttexttotexturetotexttexttotexttexttotexttexttotexttexttotexttexttotexttexttotexttexttotexttexttotexttexttotexttexttotexttexttotexttexttotexttexttotexttexttotexttexttotexttexttotexttexttotexttexttotexttexttotexttexttotexttexttotexttexttotext
[serge]: 2023-06-27T18:28:55.731813+0000 INFO Hi, how can I help you? ⁇  ⁇  ⁇ nopeaktexttoskreplytexttoreplytexttotexttexttotexturetotexttexttotexttexttotexttexttotexttexttotexttexttotexttexttotexttexttotexttexttotexttexttotexttexttotexttexttotexttexttotexttexttotexttexttotexttexttotexttexttotexttexttotexttexttotexttexttotexttexttotexttexttotexttexttotext
[serge]: Exception ignored in: <generator object stream_ask_a_question.<locals>.event_generator at 0x7fbca44bf240>
[serge]: Traceback (most recent call last):
[serge]:   File "/usr/local/lib/python3.11/site-packages/anyio/_backends/_asyncio.py", line 802, in run
[serge]:     context, func, args, future = item
[serge]:                    ^^^^
[serge]: RuntimeError: generator ignored GeneratorExit
[serge]: 2023-06-27 18:28:55.732 | DEBUG    | serge.routers.chat:stream_ask_a_question:143 - Starting redis client
[serge]: 2023-06-27T18:28:55.732366+0000 DEBUG Starting redis client
[serge]: 2023-06-27 18:28:55.732 | DEBUG    | serge.routers.chat:stream_ask_a_question:149 - creating chat
[serge]: 2023-06-27T18:28:55.732769+0000 DEBUG creating chat
[serge]: 2023-06-27 18:28:55.733 | DEBUG    | serge.routers.chat:stream_ask_a_question:153 - model_path='GPT4AlpacaLoRA-30B' n_ctx=512 n_threads=4 last_n_tokens_size=64 max_tokens=256 temperature=0.1 top_p=0.95 repeat_penalty=1.3 top_k=50
[serge]: 2023-06-27T18:28:55.733064+0000 DEBUG model_path='GPT4AlpacaLoRA-30B' n_ctx=512 n_threads=4 last_n_tokens_size=64 max_tokens=256 temperature=0.1 top_p=0.95 repeat_penalty=1.3 top_k=50
[serge]: 2023-06-27 18:28:55.733 | DEBUG    | serge.routers.chat:stream_ask_a_question:154 - creating history
[serge]: 2023-06-27T18:28:55.733149+0000 DEBUG creating history
[serge]: 2023-06-27 18:28:55.733 | DEBUG    | serge.routers.chat:stream_ask_a_question:157 - adding question How are you?
[serge]: 
[serge]: 2023-06-27T18:28:55.733276+0000 DEBUG adding question How are you?
[serge]: 
[serge]: 2023-06-27 18:28:55.733 | DEBUG    | serge.routers.chat:stream_ask_a_question:163 - creating Llama client
[serge]: 2023-06-27T18:28:55.733681+0000 DEBUG creating Llama client
[serge]: llama.cpp: loading model from /usr/src/app/weights/GPT4AlpacaLoRA-30B.bin
[serge]: llama_model_load_internal: format     = ggjt v3 (latest)
[serge]: llama_model_load_internal: n_vocab    = 32000
[serge]: llama_model_load_internal: n_ctx      = 512
[serge]: llama_model_load_internal: n_embd     = 6656
[serge]: llama_model_load_internal: n_mult     = 256
[serge]: llama_model_load_internal: n_head     = 52
[serge]: llama_model_load_internal: n_layer    = 60
[serge]: llama_model_load_internal: n_rot      = 128
[serge]: llama_model_load_internal: ftype      = 9 (mostly Q5_1)
[serge]: llama_model_load_internal: n_ff       = 17920
[serge]: llama_model_load_internal: n_parts    = 1
[serge]: llama_model_load_internal: model size = 30B
[serge]: llama_model_load_internal: ggml ctx size =    0.13 MB
[serge]: llama_model_load_internal: mem required  = 25573.14 MB (+ 3124.00 MB per state)
[serge]: .
[serge]: llama_init_from_file: kv self size  =  780.00 MB
[serge]: AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
[serge]: INFO:     10.42.0.10:43030 - "GET /chat/18773ede-d500-400f-a022-b0944bd848b0/question?prompt=How+are+you%3F%0A HTTP/1.1" 200 OK
[serge]: 
[serge]: llama_print_timings:        load time = 12598.07 ms
[serge]: llama_print_timings:      sample time =     4.46 ms /    18 runs   (    0.25 ms per token)
[serge]: llama_print_timings: prompt eval time = 12598.05 ms /    44 tokens (  286.32 ms per token)
[serge]: llama_print_timings:        eval time =  8793.68 ms /    17 runs   (  517.28 ms per token)
[serge]: llama_print_timings:       total time = 21646.59 ms
[serge]: 2023-06-27 18:29:17.954 | INFO     | serge.routers.chat:event_generator:205 - I am doing well, thank you for asking! ⁇ ' ⁇ noinput` #>
[serge]: 2023-06-27T18:29:17.954839+0000 INFO I am doing well, thank you for asking! ⁇ ' ⁇ noinput` #>
[serge]: INFO:     10.42.0.10:43030 - "GET /chat/ HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:35042 - "GET /chat/18773ede-d500-400f-a022-b0944bd848b0 HTTP/1.1" 200 OK
[serge]: 2023-06-27 18:29:33.976 | DEBUG    | serge.routers.chat:stream_ask_a_question:143 - Starting redis client
[serge]: 2023-06-27T18:29:33.976730+0000 DEBUG Starting redis client
[serge]: 2023-06-27 18:29:33.978 | DEBUG    | serge.routers.chat:stream_ask_a_question:149 - creating chat
[serge]: 2023-06-27T18:29:33.978029+0000 DEBUG creating chat
[serge]: 2023-06-27 18:29:33.978 | DEBUG    | serge.routers.chat:stream_ask_a_question:153 - model_path='GPT4AlpacaLoRA-30B' n_ctx=512 n_threads=4 last_n_tokens_size=64 max_tokens=256 temperature=0.1 top_p=0.95 repeat_penalty=1.3 top_k=50
[serge]: 2023-06-27T18:29:33.978686+0000 DEBUG model_path='GPT4AlpacaLoRA-30B' n_ctx=512 n_threads=4 last_n_tokens_size=64 max_tokens=256 temperature=0.1 top_p=0.95 repeat_penalty=1.3 top_k=50
[serge]: 2023-06-27 18:29:33.978 | DEBUG    | serge.routers.chat:stream_ask_a_question:154 - creating history
[serge]: 2023-06-27T18:29:33.978956+0000 DEBUG creating history
[serge]: 2023-06-27 18:29:33.979 | DEBUG    | serge.routers.chat:stream_ask_a_question:157 - adding question What is you name?
[serge]: 2023-06-27T18:29:33.979401+0000 DEBUG adding question What is you name?
[serge]: 2023-06-27 18:29:33.980 | DEBUG    | serge.routers.chat:stream_ask_a_question:163 - creating Llama client
[serge]: 2023-06-27T18:29:33.980484+0000 DEBUG creating Llama client
[serge]: llama.cpp: loading model from /usr/src/app/weights/GPT4AlpacaLoRA-30B.bin
[serge]: llama_model_load_internal: format     = ggjt v3 (latest)
[serge]: llama_model_load_internal: n_vocab    = 32000
[serge]: llama_model_load_internal: n_ctx      = 512
[serge]: llama_model_load_internal: n_embd     = 6656
[serge]: llama_model_load_internal: n_mult     = 256
[serge]: llama_model_load_internal: n_head     = 52
[serge]: llama_model_load_internal: n_layer    = 60
[serge]: llama_model_load_internal: n_rot      = 128
[serge]: llama_model_load_internal: ftype      = 9 (mostly Q5_1)
[serge]: llama_model_load_internal: n_ff       = 17920
[serge]: llama_model_load_internal: n_parts    = 1
[serge]: llama_model_load_internal: model size = 30B
[serge]: llama_model_load_internal: ggml ctx size =    0.13 MB
[serge]: llama_model_load_internal: mem required  = 25573.14 MB (+ 3124.00 MB per state)
[serge]: .
[serge]: llama_init_from_file: kv self size  =  780.00 MB
[serge]: AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
[serge]: INFO:     10.42.0.10:40002 - "GET /chat/18773ede-d500-400f-a022-b0944bd848b0/question?prompt=What+is+you+name%3F HTTP/1.1" 200 OK
[serge]: 
[serge]: llama_print_timings:        load time = 24538.37 ms
[serge]: llama_print_timings:      sample time =     5.12 ms /    20 runs   (    0.26 ms per token)
[serge]: llama_print_timings: prompt eval time = 24538.35 ms /    85 tokens (  288.69 ms per token)
[serge]: llama_print_timings:        eval time =  9956.26 ms /    19 runs   (  524.01 ms per token)
[serge]: llama_print_timings:       total time = 34783.20 ms
[serge]: 2023-06-27 18:30:09.230 | INFO     | serge.routers.chat:event_generator:205 - My name is Ai. It's nice to meet you. How about yours? #>
[serge]: 2023-06-27T18:30:09.230001+0000 INFO My name is Ai. It's nice to meet you. How about yours? #>
[serge]: INFO:     10.42.0.10:40002 - "GET /chat/ HTTP/1.1" 200 OK
[serge]: INFO:     10.42.0.10:37010 - "GET /chat/18773ede-d500-400f-a022-b0944bd848b0 HTTP/1.1" 200 OK
