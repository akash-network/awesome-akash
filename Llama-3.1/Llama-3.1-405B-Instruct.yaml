---
version: "2.0"
services:
  rayhead:
    image: vllm/vllm-openai:v0.5.3.post1
    expose:
      - port: 8000
        as: 8000
        to:
          - global: true
      - port: 8265
        as: 8265
        to: 
          - service: rayworker
          - global: true
      - port: 6380
        as: 6380
        to:
          - service: rayworker
          - global: true

      - port: 8078
        as: 8078
        to:
          - service: rayworker
      - port: 8079
        as: 8079
        to:
          - service: rayworker
      - port: 10002
        as: 10002
        to:
          - service: rayworker
      - port: 10003
        as: 10003
        to:
          - service: rayworker
      - port: 10004
        as: 10004
        to:
          - service: rayworker
      - port: 10005
        as: 10005
        to:
          - service: rayworker
      - port: 10006
        as: 10006
        to:
          - service: rayworker
      - port: 10007
        as: 10007
        to:
          - service: rayworker
      - port: 10008
        as: 10008
        to:
          - service: rayworker
      - port: 10009
        as: 10009
        to:
          - service: rayworker
      - port: 10010
        as: 10010
        to:
          - service: rayworker
    command:
      - bash
      - "-c"
    args:
      - >-
        ray start --head --port=6380 
        --dashboard-port=8265 --dashboard-host=0.0.0.0 
        --object-manager-port=8076 --node-manager-port=8077 
        --dashboard-agent-grpc-port=8078 --dashboard-agent-listen-port=8079 
        --min-worker-port=10002 --min-worker-port=10010 --block &
        echo "Waiting for Ray to start" && 
        sleep 15 && 
        vllm serve meta-llama/Meta-Llama-3.1-405B-Instruct --tensor-parallel-size 8 --pipeline-parallel-size 3 --gpu-memory-utilization 0.99 --enable-chunked-prefill=False
    env:
      - HF_TOKEN=hf_                          # Hugging Face API token required for Meta-Llama Models
      #- NCCL_DEBUG=INFO # Uncomment to enable NCCL debugging
    params:
      storage:
        shm:
          mount: /dev/shm
        data:
          mount: /root/.cache # Mount the data storage to the cache directory for persistent storage of model files
          readOnly: false
  rayworker:
    image: vllm/vllm-openai:v0.5.3.post1
    expose:
      - port: 8078
        as: 8078
        to:
          - global: true
      - port: 8079
        as: 8079
        to:
          - global: true
    command:
      - bash
      - "-c"
    args:
      - >- 
        ray start --address $RAYHEAD_PORT_6380_TCP_ADDR:6380 --min-worker-port=10002 --min-worker-port=10010 --block
    env:
      - HF_TOKEN=hf_                          # Hugging Face API token required for Meta-Llama Models
    #  - NCCL_DEBUG=INFO
    params:
      storage:
        shm:
          mount: /dev/shm
        data:
          mount: /root/.cache # Mount the data storage to the cache directory for persistent storage of model files
          readOnly: false
  rayworker2:
    image: vllm/vllm-openai:v0.5.3.post1
    expose:
      - port: 8078
        as: 8078
        to:
          - global: true
      - port: 8079
        as: 8079
        to:
          - global: true
    command:
      - bash
      - "-c"
    args:
      - >- 
        ray start --address $RAYHEAD_PORT_6380_TCP_ADDR:6380 --min-worker-port=10002 --min-worker-port=10010 --block
    env:
      - HF_TOKEN=hf_                          # Hugging Face API token required for Meta-Llama Models
      #- NCCL_DEBUG=INFO
    params:
      storage:
        shm:
          mount: /dev/shm
        data:
          mount: /root/.cache # Mount the data storage to the cache directory for persistent storage of model files
          readOnly: false
profiles:
  compute:
    rayhead:
      resources:
        cpu:
          units: 128
        memory:
          size: 128Gi
        storage:
          - size: 100Gi
          - name: data
            size: 1000Gi
            attributes:
              persistent: true
              class: beta3
          - name: shm
            size: 10Gi
            attributes:
              class: ram
              persistent: false
        gpu:
          units: 8
          attributes:
            vendor:
              nvidia:
                - model: h100
                  ram: 80Gi
                - model: a100
                  ram: 80Gi
    rayworker:
      resources:
        cpu:
          units: 128
        memory:
          size: 128Gi
        storage:
          - size: 100Gi
          - name: data
            size: 1000Gi
            attributes:
              persistent: true
              class: beta3
          - name: shm
            size: 10Gi
            attributes:
              class: ram
              persistent: false
        gpu:
          units: 8
          attributes:
            vendor:
              nvidia:
                - model: h100
                  ram: 80Gi
                - model: a100
                  ram: 80Gi
    rayworker2:
      resources:
        cpu:
          units: 128
        memory:
          size: 128Gi
        storage:
          - size: 100Gi
          - name: data
            size: 1000Gi
            attributes:
              persistent: true
              class: beta3
          - name: shm
            size: 10Gi
            attributes:
              class: ram
              persistent: false
        gpu:
          units: 8
          attributes:
            vendor:
              nvidia:
                - model: h100
                  ram: 80Gi
                - model: a100
                  ram: 80Gi
  placement:
    dcloud:
      pricing:
        rayhead:
          denom: uakt
          amount: 1000000
        rayworker:
          denom: uakt
          amount: 1000
        rayworker2:
          denom: uakt
          amount: 1000
deployment:
  rayhead:
    dcloud:
      profile: rayhead
      count: 1
  rayworker:
    dcloud:
      profile: rayworker
      count: 1
  rayworker2:
    dcloud:
      profile: rayworker2
      count: 1