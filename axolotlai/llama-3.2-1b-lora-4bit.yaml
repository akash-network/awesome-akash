base_model: "NousResearch/Llama-3.2-1B"
load_in_4bit: true
adapter: "lora"
sequence_len: 2048

datasets:
  - path: "teknium/GPT4-LLM-Cleaned"
    type: alpaca
    split: "train[:1000]"

lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ["q_proj","v_proj","k_proj","o_proj","gate_proj","up_proj","down_proj"]

gradient_checkpointing: true
micro_batch_size: 2
gradient_accumulation_steps: 8
epochs: 1
optimizer: "adamw_torch"
learning_rate: 2e-4
lr_scheduler: "cosine"
warmup_ratio: 0.03
bf16: true

output_dir: "/workspace/data/outputs/quickstart"
hub_model_id: "your-hf-username/llama-3.2-1b-quickstart"