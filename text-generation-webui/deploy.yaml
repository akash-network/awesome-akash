version: "2.0"
services:
  text-generation-webui:
    # v1.4 of text-generation-webui change this if you want to use a newer/older version
    image: zjuuu/text-generation-webui:v1.4
    expose:
      # ui port
      - port: 7860
        as: 80
        to:
          - global: true
      # api port
      - port: 5000
        as: 5000
        to:
          - global: true
      # api-stream port
      - port: 5005
        as: 5005
        to:
          - global: true
    env:
      # by default the Dockerfile specifies these versions: 3.5;5.0;6.0;6.1;7.0;7.5;8.0;8.6+PTX
      # however for me to work i had to specify the exact version for my card ( 2060 ) it was 7.5
      # https://developer.nvidia.com/cuda-gpus you can find the version for your card here
      - TORCH_CUDA_ARCH_LIST=7.5
      - CLI_ARGS=--listen --auto-devices --api
      # the following examples have been tested with the files linked in docs/README_docker.md:
      # example running 13b with 4bit/128 groupsize        : CLI_ARGS=--model llama-13b-4bit-128g --wbits 4 --listen --groupsize 128 --pre_layer 25
      # example with loading api extension and public share: CLI_ARGS=--model llama-7b-4bit --wbits 4 --listen --auto-devices --no-stream --extensions api --share
      # example running 7b with 8bit groupsize             : CLI_ARGS=--model llama-7b --load-in-8bit --listen --auto-devices
      # the port the webui binds to on the host
      - HOST_PORT=7860
      # the port the webui binds to inside the container
      - CONTAINER_PORT=7860
      # the port the api binds to on the host
      - HOST_API_PORT=5000
      # the port the api binds to inside the container
      - CONTAINER_API_PORT=5000
      # the port the api stream endpoint binds to on the host
      - HOST_API_STREAM_PORT=5005
      # the port the api stream endpoint binds to inside the container
      - CONTAINER_API_STREAM_PORT=5005
      # the version used to install text-generation-webui from
      - WEBUI_VERSION=HEAD
profiles:
  compute:
    text-generation-webui:
      resources:
        cpu:
          units: 12
        memory:
          size: 32Gi
        gpu:
          units: 1
          attributes:
            vendor:
              nvidia:
                # some models require a lot of vram to run please research the requirements of the model you want to deploy and use a capable gpu
                # uncomment or leave it empty to receive bids with any available gpu
                #- model: a40
                #- model: a10
                #- model: 4090
                #- model: 3090Ti
                #- model: 4080
                #- model: 3090
                #- model: h100
                #- model: a100
                #- model: v100
                #- model: 3060
                #- model: p100
                #- model: 4000
                #- model: a4000
                #- model: a6000
                #- model: 2070
                #- model: 1080
                #- model: 1080Ti
        # Size of the storage depends on the models you want to download
        storage:
          - size: 300Gi
  placement:
    westcoast:
      pricing:
        text-generation-webui:
          denom: uakt
          amount: 10000000
deployment:
  text-generation-webui:
    westcoast:
      profile: text-generation-webui
      count: 1
